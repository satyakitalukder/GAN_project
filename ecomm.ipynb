{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca11426e-8f6c-42d2-9f69-dbb21b1e832c",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n",
    "For the sake of example, let's assume we have a dataset of product images. We'll use the CIFAR-10 dataset, which can be replaced with your actual product images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f031709c-8b33-4e64-b277-6e4f7a2f7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 4us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train / 127.5 - 1.0  # Normalize to [-1, 1]\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "# Constants\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "CHANNELS = 3\n",
    "IMG_SHAPE = (IMG_ROWS, IMG_COLS, CHANNELS)\n",
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2793161-828f-4621-9ebd-53849572e51e",
   "metadata": {},
   "source": [
    "### Step 2: Building the Generator\n",
    "The generator will take random noise as input and generate product images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa6441f-0487-469d-b898-fd6faf94f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256 * 8 * 8, activation=\"relu\", input_dim=LATENT_DIM))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(CHANNELS, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM,))\n",
    "    img = model(noise)\n",
    "    \n",
    "    return Model(noise, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5772c-f3aa-4135-997e-0b43a9aead54",
   "metadata": {},
   "source": [
    "### Step 3: Building the Discriminator\n",
    "The discriminator will classify images as real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c085f92d-b0c4-460c-9535-a51630906c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=IMG_SHAPE, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    img = Input(shape=IMG_SHAPE)\n",
    "    validity = model(img)\n",
    "    \n",
    "    return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dc989-48f5-4cfd-a121-05f91f4ae455",
   "metadata": {},
   "source": [
    "### Step 4: Compile the GAN\n",
    "Combine the generator and discriminator to create the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cdbb9d-b64f-493c-92dd-384a7e804aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM,))\n",
    "    img = generator(noise)\n",
    "    validity = discriminator(img)\n",
    "    \n",
    "    gan = Model(noise, validity)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    \n",
    "    return gan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c60b7-f0bc-4779-9fbb-fa0d55dee7ca",
   "metadata": {},
   "source": [
    "### Step 5: Training the GAN\n",
    "Train the GAN by alternating between training the discriminator and the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511e636-416c-4cf6-81a2-9d3828ce0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(epochs, batch_size=128, save_interval=50):\n",
    "    X_train = (X_train / 127.5) - 1.0\n",
    "    \n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, LATENT_DIM))\n",
    "        g_loss = gan.train_on_batch(noise, valid)\n",
    "        \n",
    "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {d_loss[1]}] [G loss: {g_loss}]\")\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, LATENT_DIM))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"images/{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Build and compile the discriminator, generator and finally the GAN\n",
    "discriminator = build_discriminator()\n",
    "generator = build_generator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "train_gan(epochs=10000, batch_size=32, save_interval=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece8974-0f51-4087-9155-e20d8705d0e1",
   "metadata": {},
   "source": [
    "### Step 6: Human Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0e85d-0355-4b10-a96b-31d7e5f0528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-pose-estimation\n",
    "\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "import cv2\n",
    "\n",
    "w, h = model_wh('432x368')\n",
    "pose_estimator = TfPoseEstimator(get_graph_path('mobilenet_thin'), target_size=(w, h))\n",
    "\n",
    "def estimate_pose(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    humans = pose_estimator.inference(image)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    return humans, image\n",
    "\n",
    "user_image_path = 'path_to_user_image.jpg'\n",
    "humans, pose_image = estimate_pose(user_image_path)\n",
    "plt.imshow(pose_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611927a6-a182-496e-9667-70c8c2bf9cb8",
   "metadata": {},
   "source": [
    "### Step 7: Clothing Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fe2af-7e31-4514-a793-f4bb888516a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained U-Net model\n",
    "unet_model = load_model('unet_clothing_segmentation.h5')\n",
    "\n",
    "def segment_clothing(clothing_image_path):\n",
    "    clothing_image = cv2.imread(clothing_image_path)\n",
    "    clothing_image = cv2.resize(clothing_image, (128, 128))\n",
    "    clothing_image = np.expand_dims(clothing_image, axis=0) / 255.0\n",
    "    segmentation_mask = unet_model.predict(clothing_image)[0]\n",
    "    segmentation_mask = (segmentation_mask > 0.5).astype(np.uint8)\n",
    "    return segmentation_mask\n",
    "\n",
    "clothing_image_path = 'path_to_clothing_image.jpg'\n",
    "segmentation_mask = segment_clothing(clothing_image_path)\n",
    "plt.imshow(segmentation_mask, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a4b5f-456f-4e00-a789-09c871c47ac2",
   "metadata": {},
   "source": [
    "### Step 8: Image Warping and Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c71df6-f49f-4083-b644-3c5b3e9845f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(humans):\n",
    "    # Example implementation, you need to extract relevant key points\n",
    "    keypoints = {}\n",
    "    for human in humans:\n",
    "        for i in range(len(human.body_parts)):\n",
    "            body_part = human.body_parts[i]\n",
    "            keypoints[i] = (int(body_part.x * IMG_COLS), int(body_part.y * IMG_ROWS))\n",
    "    return keypoints\n",
    "\n",
    "def warp_clothing_to_body(humans, clothing_image, segmentation_mask):\n",
    "    # Extract key points from pose estimation\n",
    "    keypoints = extract_keypoints(humans)\n",
    "    \n",
    "    # Placeholder for actual implementation of transformation_matrix calculation\n",
    "    transformation_matrix = np.eye(2, 3)  # Identity matrix as placeholder\n",
    "    \n",
    "    # Warp the clothing image using the key points\n",
    "    warped_clothing = cv2.warpAffine(clothing_image, transformation_matrix, (clothing_image.shape[1], clothing_image.shape[0]))\n",
    "    warped_clothing_masked = cv2.bitwise_and(warped_clothing, warped_clothing, mask=segmentation_mask)\n",
    "    return warped_clothing_masked\n",
    "\n",
    "# Example usage\n",
    "clothing_image = cv2.imread(clothing_image_path)\n",
    "warped_clothing = warp_clothing_to_body(humans, clothing_image, segmentation_mask)\n",
    "plt.imshow(warped_clothing)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301d57f-6632-48ea-a899-b78d87852746",
   "metadata": {},
   "source": [
    "### After completing the individual steps of the process, the final integration involves:\n",
    "\n",
    "Training the GAN to generate new clothing images.\n",
    "Using pose estimation on the user's image.\n",
    "Segmenting the clothing from the generated images.\n",
    "Warping the clothing to fit the user's pose.\n",
    "Overlaying the warped clothing onto the user's image.\n",
    "Here are the steps to accomplish this integration:\n",
    "\n",
    "### 1. Train the GAN (INTEGRATION)\n",
    "Make sure the GAN is trained sufficiently to generate diverse and high-quality clothing images. You should already have this from the previous steps.\n",
    "\n",
    "### 2. Estimate Pose on User Image(INTEGRATION)\n",
    "Run the pose estimation on the user's image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c92bb-27af-4fb4-8693-eb87b3f00397",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_image_path = 'path_to_user_image.jpg'\n",
    "humans, pose_image = estimate_pose(user_image_path)\n",
    "plt.imshow(pose_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9607d3-9668-4536-b283-89f3ebff25b5",
   "metadata": {},
   "source": [
    "### 3. Generate New Clothing Image\n",
    "Use the trained generator to create a new clothing image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f736a7-a235-4f33-98e0-08a333675a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (1, LATENT_DIM))\n",
    "gen_clothing_img = generator.predict(noise)[0]\n",
    "gen_clothing_img = 0.5 * gen_clothing_img + 0.5  # Rescale to [0, 1]\n",
    "\n",
    "plt.imshow(gen_clothing_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4d767-139e-4bda-95c7-1a8462ee778c",
   "metadata": {},
   "source": [
    "### 4. Segment the Generated Clothing Image\n",
    "Segment the generated clothing image to isolate the clothing item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50347ffe-24ec-45a2-9e6b-a417914590ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_clothing_img_resized = cv2.resize(gen_clothing_img, (128, 128))\n",
    "gen_clothing_img_resized = np.expand_dims(gen_clothing_img_resized, axis=0)\n",
    "segmentation_mask = unet_model.predict(gen_clothing_img_resized)[0]\n",
    "segmentation_mask = (segmentation_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "plt.imshow(segmentation_mask, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93ff00-1372-4bd4-8815-a7b64c4301c4",
   "metadata": {},
   "source": [
    "### 5. Warp the Clothing Image to Fit User's Pose\n",
    "Warp the segmented clothing image to fit the user's pose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b5038-6aa6-4f27-9078-986ee2017d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_clothing_img_cv2 = (gen_clothing_img * 255).astype(np.uint8)\n",
    "warped_clothing = warp_clothing_to_body(humans, gen_clothing_img_cv2, segmentation_mask)\n",
    "\n",
    "plt.imshow(warped_clothing)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e5f2f-8e95-4255-961c-e8c86af11688",
   "metadata": {},
   "source": [
    "### 6. Overlay the Warped Clothing onto the User's Image\n",
    "Overlay the warped clothing onto the user's image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8e0e2-3305-4bba-83a9-03a544111f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_clothing_on_user(user_image_path, warped_clothing):\n",
    "    user_image = cv2.imread(user_image_path)\n",
    "    user_image_resized = cv2.resize(user_image, (warped_clothing.shape[1], warped_clothing.shape[0]))\n",
    "    \n",
    "    overlay_image = user_image_resized.copy()\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    for c in range(0, 3):\n",
    "        overlay_image[:, :, c] = (alpha * warped_clothing[:, :, c] + (1 - alpha) * user_image_resized[:, :, c])\n",
    "    \n",
    "    return overlay_image\n",
    "\n",
    "user_image_with_clothing = overlay_clothing_on_user(user_image_path, warped_clothing)\n",
    "plt.imshow(cv2.cvtColor(user_image_with_clothing, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
